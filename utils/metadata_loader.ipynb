{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subreddit(master_df: pd.DataFrame, name: str):\n",
    "    \"\"\"\n",
    "    Process a subreddit's data from the master DataFrame.\n",
    "    Handles missing values and data inconsistencies.\n",
    "    \"\"\"\n",
    "    # Filter and clean data\n",
    "    subreddit_comments = master_df[master_df[\"subreddit\"] == name].copy()\n",
    "    \n",
    "    # Convert empty strings to NaN\n",
    "    for col in ['text_sentiment', 'toxic_word_count', 'left_wing', 'righy_wing']:\n",
    "        subreddit_comments[col] = subreddit_comments[col].replace('', np.nan)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(f\"data/processed/{name}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # --- Generate Metadata ---\n",
    "    metadata = {\n",
    "        # Sentiment analysis (handle missing values)\n",
    "        \"sentiment_avg\": subreddit_comments[\"text_sentiment\"].mean(skipna=True),\n",
    "        \"sentiment_distribution\": {\n",
    "            \"positive\": (subreddit_comments[\"text_sentiment\"] > 0.3).mean(),\n",
    "            \"neutral\": ((subreddit_comments[\"text_sentiment\"] >= -0.3) & \n",
    "                       (subreddit_comments[\"text_sentiment\"] <= 0.3)).mean(),\n",
    "            \"negative\": (subreddit_comments[\"text_sentiment\"] < -0.3).mean()\n",
    "        },\n",
    "        \n",
    "        # Toxicity analysis (handle missing toxic_word_count)\n",
    "        \"toxicity_avg\": subreddit_comments[\"toxic_word_count\"].mean(skipna=True),\n",
    "        \"toxic_comments_ratio\": (subreddit_comments[\"toxic_word_count\"] > 0).mean(),\n",
    "        \n",
    "        # Political lean (correcting 'righy_wing' typo and handling predicted_label)\n",
    "        \"political_lean\": {\n",
    "            \"left\": subreddit_comments[\"left_wing\"].mean(skipna=True),\n",
    "            \"right\": subreddit_comments[\"righy_wing\"].mean(skipna=True),\n",
    "            # Fallback to predicted_label if wing counts missing\n",
    "            \"predicted_left\": (subreddit_comments[\"predicted_label\"] == \"Left\").mean(),\n",
    "            \"predicted_right\": (subreddit_comments[\"predicted_label\"] == \"Right\").mean()\n",
    "        },\n",
    "        \n",
    "        # Engagement metrics\n",
    "        \"avg_upvotes\": subreddit_comments[\"upvotes\"].mean(),\n",
    "        \"total_comments\": len(subreddit_comments)\n",
    "    }\n",
    "    \n",
    "    print(\"Saving to: \", output_dir / \"metadata.json\")\n",
    "    \n",
    "    # Save metadata\n",
    "    try: \n",
    "        with open(output_dir / \"metadata.json\", \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    except TypeError as e:\n",
    "        print(\"Failed to serialize metadata: \", e)\n",
    "    except Exception as e:\n",
    "        print(\"Error saving metadata: \", e)\n",
    "    \n",
    "    # --- Generate Word Clouds ---\n",
    "    wc_params = {\n",
    "        \"width\": 1200,\n",
    "        \"height\": 800,\n",
    "        \"background_color\": \"white\",\n",
    "        \"max_words\": 200,\n",
    "        \"collocations\": False\n",
    "    }\n",
    "    \n",
    "    # Generate for left/right based on predicted_label\n",
    "    for lean in [\"Left\", \"Right\"]:\n",
    "        try:\n",
    "            lean_comments = subreddit_comments[\n",
    "                subreddit_comments[\"predicted_label\"] == lean\n",
    "            ]\n",
    "            if len(lean_comments) > 0:\n",
    "                text = \" \".join(lean_comments[\"preprocessed_text\"].dropna().astype(str))\n",
    "                WordCloud(**wc_params).generate(text).to_file(\n",
    "                    output_dir / f\"wordcloud_{lean.lower()}.png\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate {lean} wordcloud for r/{name}: {str(e)}\")\n",
    "    \n",
    "    # --- Generate Sentiment Timeline ---\n",
    "    if \"created_utc\" in subreddit_comments.columns:\n",
    "        try:\n",
    "            subreddit_comments[\"date\"] = pd.to_datetime(\n",
    "                subreddit_comments[\"created_utc\"], \n",
    "                unit='s'\n",
    "            ).dt.date\n",
    "            timeline = subreddit_comments.groupby(\"date\")[\"text_sentiment\"].mean()\n",
    "            timeline.to_csv(output_dir / \"sentiment_timeline.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate timeline for r/{name}: {str(e)}\")\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing r/news\n",
      "Saving to:  data/processed/news/metadata.json\n",
      "Processing r/worldnews\n",
      "Saving to:  data/processed/worldnews/metadata.json\n",
      "Processing r/politics\n",
      "Saving to:  data/processed/politics/metadata.json\n",
      "Processing r/democrats\n",
      "Saving to:  data/processed/democrats/metadata.json\n",
      "Processing r/conservative\n",
      "Saving to:  data/processed/conservative/metadata.json\n",
      "Processing r/canada\n",
      "Saving to:  data/processed/canada/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Load master dataframe\n",
    "master_df = pd.read_csv(\"utils/complete_final_toxicity_classifier.csv\")\n",
    "\n",
    "subreddit_list = [\"news\", \"worldnews\", \"politics\", \"democrats\", \n",
    "                  \"conservative\", \"canada\"]\n",
    "\n",
    "# Process all subreddits\n",
    "for subreddit in subreddit_list:\n",
    "    print(f\"Processing r/{subreddit}\")\n",
    "    process_subreddit(master_df, subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cassandra/Documents/GitHub/reddit-dashboard'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
